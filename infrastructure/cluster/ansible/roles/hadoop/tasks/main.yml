#
# Copyright 2021 Merck & Co., Inc. Kenilworth, NJ, USA.
#
#	Licensed to the Apache Software Foundation (ASF) under one
#	or more contributor license agreements. See the NOTICE file
#	distributed with this work for additional information
#	regarding copyright ownership. The ASF licenses this file
#	to you under the Apache License, Version 2.0 (the
#	"License"); you may not use this file except in compliance
#	with the License. You may obtain a copy of the License at
#
#	http://www.apache.org/licenses/LICENSE-2.0
#
#
#	Unless required by applicable law or agreed to in writing,
#	software distributed under the License is distributed on an
#	"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
#	KIND, either express or implied. See the License for the
#	specific language governing permissions and limitations
#	under the License.
#
- name: "Create Hadoop installation directory"
  file:
    path: "{{ hadoop_base_path }}"
    state: directory
    mode: "0755"
    owner: root
    group: root
  become: true

- name: Download Hadoop tarball from S3
  aws_s3:
    bucket: "{{ s3_bucket }}"
    object: "/{{ s3_sources_folder }}/hadoop/{{ hadoop_version }}/{{ hadoop_tarball }}"
    dest: "/tmp/{{ hadoop_tarball }}"
    mode: get

- name: Install Hadoop from tarball
  unarchive: 
    src: "/tmp/{{ hadoop_tarball }}"
    dest: "{{ hadoop_base_path }}"
    creates: "{{ hadoop_base_path }}/{{ hadoop_version_name }}"
    remote_src: yes
    owner: root
    group: root
  become: true

- name: "Symlink to latest version of Hadoop"
  file:
    src: "{{ hadoop_base_path }}/{{ hadoop_version_name }}"
    dest: "{{ hadoop_home }}"
    state: link
    owner: root
    group: root
  become: true

- name: "configure hadoop with templates"
  template: src={{ item }} dest={{ hadoop_home }}/etc/hadoop/{{ item }}
  with_items:
    - core-site.xml
    - hdfs-site.xml
    - mapred-site.xml
    - hadoop-env.sh
    - mapred-env.sh

- name: "configure hadoop 2"
  become: yes
  template: src={{ item }} dest={{ hadoop_home }}/etc/hadoop/{{ item }}
  with_items:
    - slaves
  when: hadoop_major_version == '2'

- name: "configure hadoop 3"
  template: src={{ item }} dest={{ hadoop_home }}/etc/hadoop/{{ item }}
  with_items:
    - workers
  when: hadoop_major_version == '3'

# This is currently needed to run hadoop with Java 11 (see https://github.com/apache/fluo-muchos/issues/266)
- name: "Copy javax.activation-api (when Hadoop 3 and Java 11 are used)"
  synchronize: src={{ user_home }}/mvn_dep/ dest={{ hadoop_home }}/share/hadoop/common/lib/
  when: hadoop_major_version == '3' and java_product_version == 11

- name: "setup hadoop short circuit socket dir"
  file: path=/var/lib/hadoop-hdfs state=directory owner=hdfs group=hdfs mode=0755
  become: yes

- name: "create hadoop data directory"
  file: path={{ worker_data_dirs[0] }}/hadoop state=directory owner=hdfs group=hdfs
  become: yes

- name: "create hadoop log directory"
  file: path={{ worker_data_dirs[0] }}/logs/hadoop state=directory owner=hdfs group=hdfs
  become: yes

- name: "create hadoop pid directory"
  file: path=/var/run/hadoop state=directory owner=hdfs group=root
  become: yes

- name: "copy hadoop pid directory creation startup script"
  copy: src=create-hadoop-pid-dir dest=/etc/init.d/create-hadoop-pid-dir mode='0755' owner=root group=root
  become: yes

- name: "install create-hadoop-pid-dir startup script"
  shell: "update-rc.d create-hadoop-pid-dir remove && update-rc.d -f create-hadoop-pid-dir defaults 99"
  become: yes

- name: "Configure hadoop log dir"
  replace:
    path: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
    regexp: '.*export\s+HADOOP_LOG_DIR.*'
    replace: "export HADOOP_LOG_DIR={{ worker_data_dirs[0] }}/logs/hadoop"

- name: "Configure JAVA_HOME in hadoop-env.sh"
  become: yes
  replace:
    path: "{{ hadoop_home}}/etc/hadoop/hadoop-env.sh"
    regexp: '.*export\s+JAVA_HOME.*'
    replace: "export JAVA_HOME={{ java_home }}"

- name: Configure HDFS systemd service template
  become: yes
  template: src=hdfs.service.j2 dest=/etc/systemd/system/hdfs@.service
